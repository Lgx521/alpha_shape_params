import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions.normal import Normal
from tqdm import tqdm
import os
import glob
from torch.utils.tensorboard import SummaryWriter



project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
save_directory = os.path.join(project_root, 'checkpoints')


# --- 1. æ ¸å¿ƒä¾èµ–å¯¼å…¥ ---
try:
    import trimesh
    print("Trimesh library found.")
except ImportError:
    print("FATAL ERROR: 'trimesh' not installed. Run: pip install trimesh")
    exit()
try:
    from torch_geometric.nn import knn_interpolate, global_max_pool, fps, radius
    from torch_geometric.data import Data, Dataset
    from torch_geometric.loader import DataLoader
    from torch_geometric.utils import to_dense_batch
    print("PyTorch Geometric found.")
except ImportError as e:
    print(f"FATAL ERROR: PyG not installed correctly. Error: {e}")
    exit()
try:
    from pytorch3d.loss import chamfer_distance, mesh_laplacian_smoothing
    from pytorch3d.ops import sample_points_from_meshes
    from pytorch3d.structures import Meshes
    print("PyTorch3D found.")
except ImportError as e:
    print("FATAL ERROR: PyTorch3D not found. Run: pip install pytorch3d")
    print(e)
    exit()
try:
    from CGAL.CGAL_Kernel import Point_3
    from CGAL.CGAL_Alpha_shape_3 import Alpha_shape_3, Mode
    print("CGAL-pybind found.")
except ImportError:
    print("WARNING: cgal-pybind not found. Reconstruction will be a DUMMY step.")


# --- 2. åŸºäºPyGçš„PointNet++ Alphaé¢„æµ‹æ¨¡å‹ (V5 - æœ€ç»ˆä¿®æ­£ç‰ˆ) ---
# è¿™ä¸ªç‰ˆæœ¬æ‹¥æœ‰ä¸€ä¸ªé€»è¾‘æ­£ç¡®ã€ç»´åº¦åŒ¹é…ã€å±‚æ¬¡åˆ†æ˜çš„U-Netæ¶æ„ã€‚
class PyG_PointNet2_Alpha_Predictor(torch.nn.Module):
    def __init__(self, k_neighbors=3):
        super().__init__()
        self.k = k_neighbors

        # å®šä¹‰ä¸€ä¸ªæ¸…æ™°çš„è¾…åŠ©å‡½æ•°æ¥åˆ›å»ºMLPå±‚
        def create_mlp(in_channels, out_channels_list):
            layers = []
            for out_channels in out_channels_list:
                layers.append(nn.Linear(in_channels, out_channels))
                layers.append(nn.ReLU(inplace=True))
                in_channels = out_channels
            # ç§»é™¤æœ€åä¸€ä¸ªReLUä»¥è·å¾—åŸå§‹çš„logits
            return nn.Sequential(*layers[:-1])

        # --- ç¼–ç å™¨ (SA) Layers ---
        # æ¯ä¸€å±‚MLPå¤„ç†çš„æ˜¯ [(ä¸Šä¸€å±‚ç‰¹å¾), (å½“å‰å±‚åæ ‡)]
        self.sa1_mlp = create_mlp(3, [64, 64, 128])
        self.sa2_mlp = create_mlp(128 + 3, [128, 128, 256])
        self.sa3_mlp = create_mlp(256 + 3, [256, 512, 1024])

        # --- è§£ç å™¨ (FP) Layers ---
        # æ¯ä¸€å±‚MLPå¤„ç†çš„æ˜¯ [(æ’å€¼åçš„ä¸Šå±‚ç‰¹å¾), (æœ¬å±‚è·³è·ƒè¿æ¥çš„ç‰¹å¾)]
        # [V5 ä¿®æ­£] ç»´åº¦ä¸forwardå‡½æ•°ä¸­çš„æ ‡å‡†U-Neté€»è¾‘å®Œå…¨åŒ¹é…
        self.fp3_mlp = create_mlp(1024 + 256, [256, 256])  # l3_up(1024) + l2_skip(256)
        self.fp2_mlp = create_mlp(256 + 128, [256, 128])   # l2_fp(256) + l1_skip(128)
        self.fp1_mlp = create_mlp(128 + 3, [128, 128, 128])# l1_fp(128) + l0_coords(3)

        # --- è¾“å‡ºå¤´ ---
        self.head_mlp = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(64, 1)
        )
        self.softplus = nn.Softplus()

    def forward(self, data):
        pos, batch = data.pos, data.batch

        # --- ç¼–ç å™¨ (Encoder) ---
        # ä¿å­˜æ¯ä¸€å±‚çš„ç‰¹å¾å’Œåæ ‡ï¼Œç”¨äºåç»­çš„è·³è·ƒè¿æ¥
        
        # Level 0 (åŸå§‹è¾“å…¥)
        l0_pos, l0_batch = pos, batch
        
        # Level 1
        l1_idx = fps(l0_pos, l0_batch, ratio=0.25)
        l1_pos, l1_batch = l0_pos[l1_idx], l0_batch[l1_idx]
        # l1_skip_features æ¥è‡ªäºå¯¹åŸå§‹åæ ‡çš„å¤„ç†
        l1_skip_features = F.relu(self.sa1_mlp(l1_pos))

        # Level 2
        l2_idx = fps(l1_pos, l1_batch, ratio=0.25)
        l2_pos, l2_batch = l1_pos[l2_idx], l1_batch[l2_idx]
        # l2_skip_features æ¥è‡ªäºå¯¹l1ç‰¹å¾çš„å¤„ç†
        l2_skip_features = F.relu(self.sa2_mlp(torch.cat([l1_skip_features[l2_idx], l2_pos], dim=1)))
        
        # Level 3 (æœ€æ·±å±‚)
        l3_idx = fps(l2_pos, l2_batch, ratio=0.25)
        l3_pos, l3_batch = l2_pos[l3_idx], l2_batch[l3_idx]
        l3_features = F.relu(self.sa3_mlp(torch.cat([l2_skip_features[l3_idx], l3_pos], dim=1)))
        
        # --- è§£ç å™¨ (Decoder) ---
        
        # FP for Level 2
        l2_interp_features = knn_interpolate(l3_features, l3_pos, l2_pos, l3_batch, l2_batch, k=self.k)
        l2_fp_features = F.relu(self.fp3_mlp(torch.cat([l2_interp_features, l2_skip_features], dim=1)))
        
        # FP for Level 1
        l1_interp_features = knn_interpolate(l2_fp_features, l2_pos, l1_pos, l2_batch, l1_batch, k=self.k)
        l1_fp_features = F.relu(self.fp2_mlp(torch.cat([l1_interp_features, l1_skip_features], dim=1)))
        
        # FP for Level 0 (åŸå§‹ç‚¹)
        l0_interp_features = knn_interpolate(l1_fp_features, l1_pos, l0_pos, l1_batch, l0_batch, k=self.k)
        # æœ€åä¸€å±‚ä¸åŸå§‹åæ ‡æ‹¼æ¥
        l0_fp_features = F.relu(self.fp1_mlp(torch.cat([l0_interp_features, l0_pos], dim=1)))

        # --- è¾“å‡ºå¤´ ---
        alpha_mean = self.head_mlp(l0_fp_features)
        
        # --- è¾“å‡ºæ ¼å¼åŒ– ---
        alpha_mean_dense, _ = to_dense_batch(alpha_mean, batch)
        alpha_mean_dense = alpha_mean_dense.permute(0, 2, 1)
        alpha_mean_activated = self.softplus(alpha_mean_dense)
        alpha_std = torch.ones_like(alpha_mean_activated) * 0.01
        policy = Normal(alpha_mean_activated, alpha_std)
        
        return policy

# --- 3. æ•°æ®åŠ è½½ (ä¿æŒä¸å˜) ---
class PyGShapeNetDataset(Dataset):
    def __init__(self, root_dir, num_points=2048, split='train'):
        self.root_dir = root_dir
        self.num_points = num_points
        self.paths = glob.glob(os.path.join(root_dir, "**/model_normalized.ply"), recursive=True)
        if not self.paths:
            raise ValueError(f"No 'model_normalized.ply' files found in {root_dir}.")
        print(f"Found {len(self.paths)} models for the '{split}' split.")
        
    def __len__(self):
        return len(self.paths)
        
    def __getitem__(self, idx):
        try:
            mesh = trimesh.load(self.paths[idx])
            verts = torch.tensor(mesh.vertices, dtype=torch.float32)
            faces = torch.tensor(mesh.faces, dtype=torch.long)
            pytorch3d_mesh = Meshes(verts=[verts], faces=[faces])
            points = sample_points_from_meshes(pytorch3d_mesh, num_samples=self.num_points)
            return Data(pos=points.squeeze(0))
        except Exception:
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ™å°è¯•åŠ è½½ä¸‹ä¸€ä¸ª
            return self.__getitem__((idx + 1) % len(self))

# --- 4. å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸å¥–åŠ± (ä¿æŒä¸å˜) ---
def reconstruct_with_alpha_shape(points, alphas):
    if 'CGAL' not in globals() or 'Alpha_shape_3' not in globals():
        return None
    median_alpha = torch.median(alphas).item()
    if median_alpha <= 1e-9:
        median_alpha = 1e-9
    try:
        points_cgal = [Point_3(p[0], p[1], p[2]) for p in points.cpu().tolist()]
        alpha_shape = Alpha_shape_3(points_cgal, median_alpha, Mode.GENERAL)
        verts_list, faces_list = alpha_shape.get_surface_mesh()
        if not verts_list or not faces_list:
            return None
        return Meshes(verts=[torch.tensor(verts_list, dtype=torch.float32)], faces=[torch.tensor(faces_list, dtype=torch.long)])
    except Exception:
        return None


def calculate_reward_v3(reconstructed_mesh, original_points, alphas, weights, device):
    """
    V3ç‰ˆå¥–åŠ±å‡½æ•°ï¼Œç»“åˆäº†ç½‘æ ¼è´¨é‡å’Œalphaå€¼æœ¬èº«çš„å±æ€§ã€‚
    """
    # --- Part 1: Alphaå€¼æœ¬èº«çš„å¯å‘å¼å¥–åŠ± (å³ä½¿ç½‘æ ¼é‡å»ºå¤±è´¥ä¹Ÿèƒ½è®¡ç®—) ---

    # 1a. å±€éƒ¨Alphaä¸€è‡´æ€§å¥–åŠ±: é¼“åŠ±å±€éƒ¨åŒºåŸŸçš„alphaå€¼å¹³æ»‘
    # æˆ‘ä»¬ä½¿ç”¨KNNæ‰¾åˆ°æ¯ä¸ªç‚¹çš„é‚»å±…ï¼Œå¹¶è®¡ç®—alphaå€¼çš„å±€éƒ¨æ–¹å·®
    with torch.no_grad():
        # ä½¿ç”¨PyGçš„radiuså‡½æ•°æ‰¾åˆ°æ¯ä¸ªç‚¹åŠå¾„èŒƒå›´å†…çš„é‚»å±…
        # è¿™ä¸ªåŠå¾„éœ€è¦æ ¹æ®ä½ çš„ç‚¹äº‘å°ºåº¦è¿›è¡Œè°ƒæ•´
        radius_graph = radius(original_points, original_points, r=0.1, max_num_neighbors=16)
        row, col = radius_graph
        # è®¡ç®—æ¯ä¸ªç‚¹ä¸å…¶é‚»å±…alphaå€¼çš„å·®çš„å¹³æ–¹çš„å‡å€¼ï¼Œä½œä¸ºå±€éƒ¨æ–¹å·®çš„ä»£ç†
        local_variance = (alphas[row] - alphas[col])**2
        # æˆ‘ä»¬å¸Œæœ›æ–¹å·®å°ï¼Œæ‰€ä»¥å¥–åŠ±æ˜¯è´Ÿæ–¹å·®
        reward_alpha_consistency = -torch.mean(local_variance)

    # 1b. Alphaå€¼å¹…åº¦æƒ©ç½š: æƒ©ç½šæç«¯å€¼
    # ä½¿ç”¨logæƒ©ç½šæ¥æ¸©å’Œåœ°æƒ©ç½šè¿‡å¤§çš„alphaå€¼
    penalty_alpha_magnitude = -torch.log(1 + torch.mean(alphas))

    # 1c. å¤šæ ·æ€§å¥–åŠ±: å¥–åŠ±alphaå€¼çš„æ ‡å‡†å·®ï¼Œé˜²æ­¢æ¨¡å‹è¾“å‡ºå¸¸æ•°
    reward_alpha_diversity = torch.std(alphas)

    # --- Part 2: åŸºäºé‡å»ºç½‘æ ¼è´¨é‡çš„å¥–åŠ± (æ ¸å¿ƒç›®æ ‡) ---
    
    reward_fidelity = -10.0  # Chamfer Loss, ä¿çœŸåº¦
    reward_smoothness = -2.0 # Laplacian Loss, å¹³æ»‘åº¦
    reward_watertight = -1.0 # æ°´å¯†æ€§
    
    if reconstructed_mesh is not None and reconstructed_mesh.verts_packed().shape[0] >= 4:
        reconstructed_mesh = reconstructed_mesh.to(device)
        try:
            # ä¿çœŸåº¦å¥–åŠ± (Chamferè·ç¦»)
            reconstructed_points = sample_points_from_meshes(reconstructed_mesh, num_samples=original_points.shape[0])
            loss_chamfer, _ = chamfer_distance(reconstructed_points, original_points.unsqueeze(0))
            # ä½¿ç”¨è´Ÿçš„Chamferè·ç¦»ä½œä¸ºå¥–åŠ±ï¼Œå¹¶è¿›è¡Œç¼©æ”¾ä»¥æ§åˆ¶å…¶å½±å“èŒƒå›´
            # reward_fidelity = -torch.clamp(loss_chamfer, 0, 10) 
            reward_fidelity = -torch.log(1.0 + torch.clamp(loss_chamfer, 0, 100))

            
            # å¹³æ»‘åº¦å¥–åŠ±
            loss_laplacian = mesh_laplacian_smoothing(reconstructed_mesh, method="uniform")
            reward_smoothness = -torch.clamp(loss_laplacian, 0, 2)

            # æ°´å¯†æ€§å¥–åŠ±
            if reconstructed_mesh.is_watertight():
                reward_watertight = 1.0 # æˆåŠŸæ„å»ºæ°´å¯†ç½‘æ ¼åº”è·å¾—æ˜¾è‘—å¥–åŠ±
        except Exception:
            # å¦‚æœåœ¨è®¡ç®—è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„æƒ©ç½šå€¼
            pass
            
    # --- Part 3: ç»„åˆæ€»å¥–åŠ± ---
    total_reward = (weights['w_fidelity'] * reward_fidelity +
                    weights['w_smoothness'] * reward_smoothness +
                    weights['w_watertight'] * reward_watertight +
                    weights['w_alpha_consistency'] * reward_alpha_consistency +
                    weights['w_alpha_magnitude'] * penalty_alpha_magnitude +
                    weights['w_alpha_diversity'] * reward_alpha_diversity)
    
    return total_reward.item()

# def calculate_reward_v4(reconstructed_mesh, original_points, weights, device):
#     """
#     V4ç‰ˆå¥–åŠ±å‡½æ•°ï¼ŒåŸºäºæ‹“æ‰‘åˆ†æï¼Œå¥–åŠ±æœ‰æ„ä¹‰çš„å‡ ä½•ç»„ä»¶ã€‚
#     """
#     # --- 1. é‡å»ºå¤±è´¥æƒ©ç½š ---
#     if reconstructed_mesh is None or reconstructed_mesh.verts_packed().shape[0] < 4:
#         return -10.0

#     reconstructed_mesh = reconstructed_mesh.to(device)

#     # --- 2. æ‹†åˆ†ç½‘æ ¼ä¸ºè¿é€šç»„ä»¶ ---
#     # æˆ‘ä»¬å·§å¦™åœ°ä½¿ç”¨to_mitsubaå‡½æ•°ï¼Œå®ƒå†…éƒ¨ä¼šè¿›è¡Œè¿é€šç»„ä»¶æ‹†åˆ†
#     # è¿™æ˜¯ä¸€ä¸ªæ¯”è‡ªå·±å†™å¾ªç¯æ›´é«˜æ•ˆã€æ›´é²æ£’çš„æ–¹æ³•
#     try:
#         # è¿™ä¸ªå‡½æ•°ä¼šè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç‹¬ç«‹è¿é€šç»„ä»¶çš„åˆ—è¡¨
#         components = to_mitsuba(reconstructed_mesh, "component")
#     except Exception:
#         # å¦‚æœæ‹†åˆ†å¤±è´¥ï¼Œè¯´æ˜ç½‘æ ¼è´¨é‡æå·®
#         return -10.0

#     if not components:
#         return -10.0

#     # --- 3. ç»„ä»¶æ•°é‡æƒ©ç½š ---
#     # å¦‚æœç½‘æ ¼è¿‡äºç ´ç¢ï¼Œç»™äºˆæ¸©å’Œæƒ©ç½šã€‚æˆ‘ä»¬ä¸å¸Œæœ›æœ‰å‡ ç™¾ä¸ªå°ç¢ç‰‡ã€‚
#     # ä½¿ç”¨logæ¥è®©æƒ©ç½šä¸è‡³äºå¤ªå‰§çƒˆ
#     reward_fragmentation = -torch.log(1.0 + torch.tensor(len(components), device=device))

#     # --- 4. æ‰¾åˆ°å¹¶åˆ†ææœ€å¤§çš„ç»„ä»¶ ---
#     # æ‰¾åˆ°åŒ…å«é¡¶ç‚¹æ•°æœ€å¤šçš„é‚£ä¸ªç»„ä»¶
#     largest_component = max(components, key=lambda m: m.verts_packed().shape[0])
    
#     # a. å°ºå¯¸å¥–åŠ±: ç›´æ¥å¥–åŠ±æœ€å¤§ç»„ä»¶çš„å°ºå¯¸
#     # æˆ‘ä»¬å¸Œæœ›æ¨¡å‹ç”Ÿæˆå¤§çš„ã€æœ‰æ„ä¹‰çš„ç»“æ„
#     reward_size = torch.log(1.0 + torch.tensor(largest_component.verts_packed().shape[0], device=device))

#     # b. ä¿çœŸåº¦å¥–åŠ± (åªé’ˆå¯¹æœ€å¤§ç»„ä»¶)
#     try:
#         component_points = sample_points_from_meshes(largest_component, num_samples=original_points.shape[0])
#         loss_chamfer, _ = chamfer_distance(component_points, original_points.unsqueeze(0))
#         # ç”¨logä»£æ›¿çº¿æ€§æƒ©ç½šï¼Œå¯¹å°çš„è¯¯å·®æ›´å®½å®¹ï¼Œå¯¹å¤§çš„è¯¯å·®æƒ©ç½šæ›´é‡
#         reward_fidelity = -torch.log(1.0 + 10.0 * loss_chamfer) 
#     except Exception:
#         reward_fidelity = -5.0 # å¦‚æœé‡‡æ ·å¤±è´¥ï¼Œç»™äºˆä¸€ä¸ªå›ºå®šæƒ©ç½š

#     # c. å¹³æ»‘åº¦å¥–åŠ± (åªé’ˆå¯¹æœ€å¤§ç»„ä»¶)
#     loss_laplacian = mesh_laplacian_smoothing(largest_component, method="uniform")
#     reward_smoothness = -torch.log(1.0 + loss_laplacian)
    
#     # --- 5. ç»„åˆæ€»å¥–åŠ± ---
#     total_reward = (weights['w_fidelity'] * reward_fidelity +
#                     weights['w_smoothness'] * reward_smoothness +
#                     weights['w_size'] * reward_size +
#                     weights['w_fragmentation'] * reward_fragmentation)

#     return total_reward.item()



def calculate_reward_v4(reconstructed_mesh, original_points, weights, device):
    """
    V4.1ç‰ˆå¥–åŠ±å‡½æ•°ï¼Œä½¿ç”¨æ‰‹åŠ¨å®ç°çš„è¿é€šç»„ä»¶æ‹†åˆ†ï¼Œä»¥å…¼å®¹æ—§ç‰ˆPyTorch3Dã€‚
    """
    # --- 1. é‡å»ºå¤±è´¥æƒ©ç½š ---
    if reconstructed_mesh is None or reconstructed_mesh.verts_packed().shape[0] < 4:
        return -10.0

    reconstructed_mesh = reconstructed_mesh.to(device)

    # --- 2. [é‡è¦æ›´æ–°] æ‰‹åŠ¨æ‹†åˆ†ç½‘æ ¼ä¸ºè¿é€šç»„ä»¶ ---
    # è¿™æ˜¯æ‚¨ROSä»£ç ä¸­ cluster_connected_triangles çš„PyTorch3Dç­‰ä»·å®ç°
    try:
        # .get_mesh_verts_faces(0) è·å–æ‰¹æ¬¡ä¸­ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰ç½‘æ ¼çš„é¡¶ç‚¹å’Œé¢
        verts = reconstructed_mesh.verts_list()[0]
        faces = reconstructed_mesh.faces_list()[0]

        # ä½¿ç”¨trimeshæ¥æ‰§è¡Œè¿é€šç»„ä»¶åˆ†æï¼Œå› ä¸ºå®ƒéå¸¸é²æ£’
        # æˆ‘ä»¬å°†PyTorch3Dçš„ç½‘æ ¼æ•°æ®ä¸´æ—¶è½¬æ¢æˆtrimeshå¯¹è±¡
        mesh_trimesh = trimesh.Trimesh(vertices=verts.cpu().numpy(), faces=faces.cpu().numpy())
        
        # split()å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç‹¬ç«‹è¿é€šç»„ä»¶çš„trimeshå¯¹è±¡åˆ—è¡¨
        components_trimesh = mesh_trimesh.split(only_watertight=False)
        
        if not components_trimesh:
            return -10.0 # å¦‚æœtrimeshæ— æ³•æ‹†åˆ†æˆ–æ‰¾ä¸åˆ°ç»„ä»¶
        
        # å°†trimeshç»„ä»¶åˆ—è¡¨è½¬æ¢å›PyTorch3Dçš„Mesheså¯¹è±¡åˆ—è¡¨
        components = []
        for comp_tm in components_trimesh:
            comp_verts = torch.tensor(comp_tm.vertices, dtype=torch.float32, device=device)
            comp_faces = torch.tensor(comp_tm.faces, dtype=torch.long, device=device)
            components.append(Meshes(verts=[comp_verts], faces=[comp_faces]))

    except Exception:
        # å¦‚æœåœ¨æ‹†åˆ†è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œè¯´æ˜ç½‘æ ¼è´¨é‡æå·®
        return -10.0

    # --- 3. ç»„ä»¶æ•°é‡æƒ©ç½š ---
    reward_fragmentation = -torch.log(1.0 + torch.tensor(len(components), device=device))

    # --- 4. æ‰¾åˆ°å¹¶åˆ†ææœ€å¤§çš„ç»„ä»¶ ---
    try:
        largest_component = max(components, key=lambda m: m.verts_packed().shape[0])
    except ValueError:
        return -10.0 # å¦‚æœç»„ä»¶åˆ—è¡¨ä¸ºç©º

    # a. å°ºå¯¸å¥–åŠ±
    reward_size = torch.log(1.0 + torch.tensor(largest_component.verts_packed().shape[0], device=device))

    # b. ä¿çœŸåº¦å¥–åŠ± (åªé’ˆå¯¹æœ€å¤§ç»„ä»¶)
    try:
        component_points = sample_points_from_meshes(largest_component, num_samples=original_points.shape[0])
        loss_chamfer, _ = chamfer_distance(component_points, original_points.unsqueeze(0))
        reward_fidelity = -torch.log(1.0 + 10.0 * loss_chamfer) 
    except Exception:
        reward_fidelity = -5.0

    # c. å¹³æ»‘åº¦å¥–åŠ± (åªé’ˆå¯¹æœ€å¤§ç»„ä»¶)
    loss_laplacian = mesh_laplacian_smoothing(largest_component, method="uniform")
    reward_smoothness = -torch.log(1.0 + loss_laplacian)
    
    # --- 5. ç»„åˆæ€»å¥–åŠ± ---
    total_reward = (weights['w_fidelity'] * reward_fidelity +
                    weights['w_smoothness'] * reward_smoothness +
                    weights['w_size'] * reward_size +
                    weights['w_fragmentation'] * reward_fragmentation)

    return total_reward.item()


# --- 5. è®­ç»ƒä¸»å‡½æ•° (V3ç‰ˆ) ---
def main():
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    SHAPENET_PATH = "/root/autodl-tmp/dataset/ShapeNetCore.v2/ShapeNetCore.v2"
    NUM_POINTS = 2048
    BATCH_SIZE = 64
    LEARNING_RATE = 0.0002
    EPOCHS = 50
    REWARD_BASELINE_DECAY = 0.95

    # # --- V3ç‰ˆå¥–åŠ±æƒé‡ (è¿™æ˜¯æ–°çš„å…³é”®è¶…å‚æ•°ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´) ---
    # REWARD_WEIGHTS_V3 = {
    #     'w_fidelity': 0.6,           # ä¸»è¦ç›®æ ‡ï¼šç½‘æ ¼ä¸ç‚¹äº‘çš„ç›¸ä¼¼åº¦
    #     'w_smoothness': 0.5,         # æ¬¡è¦ç›®æ ‡ï¼šç½‘æ ¼è¡¨é¢å¹³æ»‘
    #     'w_watertight': 1.0,         # é‡è¦ç›®æ ‡ï¼šç½‘æ ¼çš„æ‹“æ‰‘æ­£ç¡®æ€§
    #     'w_alpha_consistency': 1.5,  # å¯å‘å¼ï¼šé¼“åŠ±alphaåœºå¹³æ»‘
    #     'w_alpha_magnitude': 0.4,    # å¯å‘å¼ï¼šæƒ©ç½šè¿‡å¤§çš„alphaå€¼
    #     'w_alpha_diversity': 1.0     # å¯å‘å¼ï¼šé¼“åŠ±æ¨¡å‹æ¢ç´¢ä¸åŒçš„alphaå€¼
    # }

    # --- V4ç‰ˆå¥–åŠ±æƒé‡ï¼Œä¸“æ³¨äºæ‹“æ‰‘å’Œä¸»è¦ç»„ä»¶ ---
    REWARD_WEIGHTS_V4 = {
        'w_fidelity': 1.5,      # ä¸»è¦ç›®æ ‡ï¼šæœ€å¤§ç»„ä»¶ä¸ç‚¹äº‘çš„ç›¸ä¼¼åº¦
        'w_smoothness': 0.5,    # æ¬¡è¦ç›®æ ‡ï¼šæœ€å¤§ç»„ä»¶çš„è¡¨é¢å¹³æ»‘
        'w_size': 1.0,          # é‡è¦ç›®æ ‡ï¼šå¥–åŠ±ç”Ÿæˆæ›´å¤§çš„ä¸»ä½“ç»“æ„
        'w_fragmentation': 0.3, # å¯å‘å¼ï¼šæ¸©å’Œåœ°æƒ©ç½šè¿‡äºç ´ç¢çš„ç½‘æ ¼
    }

    # è®¾ç½®è¦åŠ è½½çš„æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™ä»å¤´è®­ç»ƒã€‚
    START_EPOCH = 0 # <-- è¯·ä¿®æ”¹ä¸ºåŠ è½½æ¨¡å‹çš„epochæ•°
    file_name = f"advanced_model_v3_epoch_{START_EPOCH}.pth"
    CHECKPOINT_PATH = os.path.join(save_directory, file_name)


    if not os.path.isdir(SHAPENET_PATH) or "/path/to/your/" in SHAPENET_PATH:
        print("="*80 + f"\nFATAL ERROR: Please update the SHAPENET_PATH variable in the code.\n" + "="*80); exit()

    # Tensorboard Visualizer
    writer = SummaryWriter('runs/adaptive_alpha_v3_experiment')
    
    model = PyG_PointNet2_Alpha_Predictor().to(DEVICE)
    dataset = PyGShapeNetDataset(root_dir=SHAPENET_PATH, num_points=NUM_POINTS)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=16, pin_memory=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    # å¼•å…¥å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå¯ä»¥è¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒ
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)


    if os.path.exists(CHECKPOINT_PATH):
        print(f"âœ… Resuming training from checkpoint: {CHECKPOINT_PATH}")
        # åŠ è½½æ¨¡å‹çš„çŠ¶æ€å­—å…¸ (æƒé‡)
        model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))
    else:
        print("ğŸŸ¡ Checkpoint file not found. Starting training from scratch.")
        # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œå°±ä»epoch 0å¼€å§‹
        START_EPOCH = 0

    reward_baseline = -5.0 # åˆå§‹åŒ–ä¸€ä¸ªæ›´ç°å®çš„åŸºçº¿

    # Tensorboard Visualizer
    global_step = 0
    
    # print(f"Starting training on {DEVICE} with V3 reward weights: {REWARD_WEIGHTS_V3}")
    print(f"Starting training on {DEVICE} with V4 reward weights: {REWARD_WEIGHTS_V4}")
    
    for epoch in range(START_EPOCH, EPOCHS):
        model.train()
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        
        # --- æ¢ç´¢è¡°å‡ ---
        # åŠ¨æ€è°ƒæ•´ç­–ç•¥çš„æ ‡å‡†å·®ï¼Œå®ç°ä»æ¢ç´¢åˆ°åˆ©ç”¨çš„è¿‡æ¸¡
        # åˆå§‹stdä¸º0.1ï¼Œæœ€ç»ˆè¡°å‡åˆ°0.01
        current_std = max(0.15 * (0.96**epoch), 0.01)

        for batch_data in progress_bar:
            batch_data = batch_data.to(DEVICE)
            points_dense, mask = to_dense_batch(batch_data.pos, batch_data.batch)
            
            # --- å‰å‘ä¼ æ’­ ---
            # ä¿®æ”¹æ¨¡å‹forwardçš„è°ƒç”¨æ–¹å¼ï¼Œä¼ å…¥std
            policy = model(batch_data) # modelçš„forwardä¸éœ€è¦æ”¹åŠ¨
            # åœ¨é‡‡æ ·å‰æ‰‹åŠ¨ä¿®æ”¹ç­–ç•¥çš„std
            policy.scale = torch.ones_like(policy.loc) * current_std 
            sampled_alphas_dense = policy.sample()

            batch_rewards = []
            for i in range(points_dense.shape[0]):
                sample_points = points_dense[i, mask[i]]
                # ä»ç¨ å¯†å¼ é‡ä¸­æå–å¯¹åº”æ ·æœ¬çš„alphaå€¼
                sample_alphas = sampled_alphas_dense[i, :, mask[i]].squeeze()

                with torch.no_grad():
                    reconstructed_mesh = reconstruct_with_alpha_shape(sample_points, sample_alphas)
                    # ä½¿ç”¨V3å¥–åŠ±å‡½æ•°
                    # reward = calculate_reward_v3(reconstructed_mesh, sample_points, sample_alphas, REWARD_WEIGHTS_V3, DEVICE)
                    reward = calculate_reward_v4(reconstructed_mesh, sample_points, REWARD_WEIGHTS_V4, DEVICE)
                    batch_rewards.append(reward)
            
            rewards_tensor = torch.tensor(batch_rewards, device=DEVICE, dtype=torch.float32)
            avg_reward = rewards_tensor.mean().item()
            advantage = rewards_tensor - reward_baseline
            
            # --- æŸå¤±è®¡ç®— (ä½¿ç”¨ä¿®æ­£åçš„æ­£ç¡®æ–¹æ³•) ---
            log_probs_dense = policy.log_prob(sampled_alphas_dense)
            log_probs_sum_per_sample = (log_probs_dense * mask.unsqueeze(1)).sum(dim=[1, 2])
            loss = - (log_probs_sum_per_sample * advantage).mean()

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            reward_baseline = REWARD_BASELINE_DECAY * reward_baseline + (1 - REWARD_BASELINE_DECAY) * avg_reward
            progress_bar.set_postfix(loss=f"{loss.item():.4f}", avg_reward=f"{avg_reward:.4f}", baseline=f"{reward_baseline:.4f}", std=f"{current_std:.3f}")

            # --- 3. <<< TENSORBOARD >>> åœ¨æ¯ä¸€æ­¥è®°å½•å…³é”®æŒ‡æ ‡ ---
            # ä½¿ç”¨ global_step ä½œä¸º X è½´ï¼Œç¡®ä¿å›¾è¡¨è¿ç»­
            writer.add_scalar('Loss/train', loss.item(), global_step)
            writer.add_scalar('Reward/average_reward', avg_reward, global_step)
            writer.add_scalar('Reward/baseline', reward_baseline, global_step)
            writer.add_scalar('Hyperparameters/learning_rate', optimizer.param_groups[0]['lr'], global_step)
            writer.add_scalar('Hyperparameters/exploration_std', current_std, global_step)
            
            # è®°å½•åˆ†å¸ƒæƒ…å†µï¼Œå¯¹äºè°ƒè¯•éå¸¸æœ‰ç”¨
            writer.add_histogram('Alphas/sampled_distribution', sampled_alphas_dense, global_step)
            writer.add_histogram('Reward/advantage_distribution', advantage, global_step)

            global_step += 1 # æ›´æ–°å…¨å±€æ­¥æ•°
        
        scheduler.step() # æ›´æ–°å­¦ä¹ ç‡
        if (epoch + 1) % 5 == 0 or (epoch + 1) == EPOCHS:
            file_name = f"advanced_model_v3_epoch_{epoch+1}.pth"
            save_path = os.path.join(save_directory, file_name)
            torch.save(model.state_dict(), save_path)

    # --- 4. <<< TENSORBOARD >>> è®­ç»ƒç»“æŸåå…³é—­writer ---
    writer.close()
    print("Training finished. TensorBoard logs saved.")

if __name__ == '__main__':
    main()